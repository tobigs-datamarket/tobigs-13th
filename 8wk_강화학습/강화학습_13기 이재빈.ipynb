{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "- 강화학습 논문 리뷰  \n",
    "    https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Atari with Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atari2006은 Reinforcement Learning에 Deep Learning을 접목시켜, 사람이 플레이하는 수준으로 구현해 낸 learning algorithm입니다.    \n",
    "**model = CNN + Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning을 그대로 RL에 적용시키기에는 몇 가지 문제가 발생합니다.   \n",
    "1. data의 Label이 존재하지 않습니다\n",
    "    - RL은 (sparse, noisy, delayed) 특성을 가진 scalar reward로부터 학습합니다   \n",
    "2. 데이터가 iid를 따르지 않습니다 (highly correlated)\n",
    "    - RL은 time series data이기 때문에, sequence들 끼리 높은 상관관계를 가지고 있습니다 \n",
    "3. 일정한 분포를 따르지 않습니다 (non-stationary dist)\n",
    "    - RL의 algorithm은 새로운 behavior를 계속적으로 학습합니다\n",
    "    \n",
    "\n",
    ".\n",
    "\n",
    "    \n",
    "따라서 여기 논문에서는, 다음과 같은 사항들을 통해 딥러닝과 강화학습을 접목시켰습니다. \n",
    "\n",
    "**CNN       \n",
    "+) Q-Learning      \n",
    "+) replay mechanism**  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"776\" alt=\"rl1\" src=\"https://user-images.githubusercontent.com/43749571/77389601-f9d2eb80-6dd6-11ea-837d-9f2b8ed4a9c4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\varepsilon$ (environment) = Atari emulator     \n",
    "구성 요소: actions, observations(화면), rewards(점수)\n",
    "\n",
    "\n",
    "1. $a_t$를 선택합니다   \n",
    "2. 선택한 action은 emulator에게 전달됩니다  \n",
    "3. state & score를 수정합니다 \n",
    "4. $r_t$ (game score)를 찾습니다  \n",
    "\n",
    "\n",
    "* 이 때, 현재 상황을 주어진 $x_t$(time t에서의 pixel 값)로만 판단하기에는 무리가 있습니다.   \n",
    "  따라서 action, observation을 보고 난 후에 game 전략을 학습합니다.    \n",
    "* 또한 게임은 언젠가 끝날 것이기 때문에, emulator의 학습 과정은 종결될 것이라고(finite) 가정하고,   \n",
    "  finite **MDP(Markov Decision Process)** 로 model을 가정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 목표는 agent가 emulator와 상호작용하여, **future reward**를 최대화하는 적절한 action을 선택하는 것입니다.   \n",
    "이를 위해, 다음과 같이 정의합니다.   \n",
    "\n",
    "* Reward Function    \n",
    "  $R_t = \\sum_{t'=t}^{T} \\gamma^(t'-t)r_t$ ($T$ = 종식 시간) \n",
    "* Q-function ($Q^*$, Optimal action-value function)    \n",
    "  $Q^*(s,a) = \\max_{\\pi}\\mathbb{E}[R_t|s_t=s, a_t=a, \\pi]$ ($\\pi$ = policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)       \n",
    "**Q-function**은 **Bellman equation**을 따르게 됩니다.    \n",
    "* $Q^*(s,a) = \\mathbb{E}_{s'~ \\varepsilon}[r + \\gamma\\max_{a'}Q^*(s,a)|s,a]$\n",
    "* 위의 식에서, expected value를 최대화하는 $a'$ 값을 선택하게 됩니다   \n",
    "* value iteration 혹은 policy iteration 과정을 거치면서,   \n",
    "  action-value function을 최적화 하게 됩니다. \n",
    "\n",
    "\n",
    "(2)     \n",
    "**Q-Network**는 Q-function을 modeling한 network이며,     \n",
    "$Q(s,a;\\theta)$ ≃ $Q^*(s,a)$입니다. (Freeze target Q-network)    \n",
    "1. 최적화\n",
    "    * 최적화의 경우, 보통 linear로 실행되며 \n",
    "    * Neural Network처럼 non-linear로 실행되는 경우도 있습니다. ($\\theta$ = weights) \n",
    "    * Q-Network는 loss function인 $L_i(\\theta_i)$ 를 최소화 하기 위한 방향으로 업데이트 됩니다.       \n",
    ".\n",
    "2. behavior distribution \n",
    "    * $\\rho(s,a)$는 s, a의 probability distribution 입니다.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **model-free**\n",
    "    - MDP의 model에 관계없이 optimal policy를 구하는 방법입니다. \n",
    "    - $\\varepsilon$을 정확하게 알고 구하는 것이 아니라,    \n",
    "      $\\varepsilon$에서 sampling 하여 action을 취해보고 정책 함수를 선정합니다.       \n",
    "\n",
    "\n",
    "\n",
    "2. **off-policy**\n",
    "    - 두 가지  policy를 동시에 사용할 경우를 말합니다. \n",
    "    - learning에 사용되는 policy는 greedy하기 improve를 하고 \n",
    "    - 움직일 때는 현재의 Q function을 토대로 $\\varepsilon$-greedy하게 움직입니다. \n",
    "    - +) behavior policy $\\mu$을 $\\varepsilon$-greedy로, target policy $\\pi$를 greedy로 택하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cf) on-policy vs off-policy**\n",
    "1. On-policy: 학습하는 policy와 행동하는 policy가 반드시 같아야만 학습이 가능한 RL 알고리즘    \n",
    "    - ex) SARSA\n",
    "    - On-policy의 경우 한번이라도 학습을 해서 policy imporvement를 시킨 순간,    \n",
    "      그 policy가 했던 과거의 experience들은 모두 사용이 불가능합니다.   \n",
    "    - 한 번 exploration을 해서 얻은 experience를 학습하고 나면 바로 재사용은 불가능합니다.   \n",
    "    - 따라서, 매우 데이터 효율성이 떨어집니다. \n",
    "   \n",
    "   \n",
    "2. Off-policy: 학습하는 policy와 행동하는 policy가 반드시 같지 않아도 학습이 가능한 RL 알고리즘 \n",
    "    - ex) Q-Learning\n",
    "    - Off-policy는 현재 학습하는 policy가 과거에 했던 experience도 학습에 사용이 가능하고,    \n",
    "    - 사람이나 다른 agent들을 통해서도 학습이 가능합니다.  \n",
    "    - exploration을 계속하면서도 optimal한 policy를 학습할 수 있습니다. \n",
    "    - 하나의 policy를 따르면서도 multiple policy를 학습할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cf) $\\varepsilon$-greedy**\n",
    "1. greedy\n",
    "    - 한 번 exploration 한 후, 최고의 보상을 받을 수 있는 action을 계속 취하는 것 \n",
    "    - ex) 한 번 플레이 한 후, 돈을 가장 많이 딴 슬롯머신에 모두 투자\n",
    "    - 수렴은 빠르지만, 충분한 탐험을 하지 않았기 때문에 local minimum에 빠질 가능성이 있습니다. \n",
    "    \n",
    "    \n",
    "2. $\\varepsilon$-greedy\n",
    "    - greedy strategy를 따르는 action을 취할 확률: $1-\\varepsilon$\n",
    "    - random action을 취할 확률: $\\varepsilon$\n",
    "    - ex) 동전을 던져서 윗면이 나오면 점수 좋았던 슬롯머신, 뒷면이 나오면 랜덤으로 선택 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TD-gammon** \n",
    "1. model-free\n",
    "2. value function을 MLP (with one hidden layer)로 계산합니다.            \n",
    "3. backgammon에서만 적용될 수도 있다는 한계가 있습니다.    \n",
    "4. **Diverge** : Q-Learning & non-linear function approximators일 때, 발산 가능성이 존재합니다.          \n",
    "   수렴성 을 보장하기 위해서는 linear function approximator & better guarantees 필요합니다.   \n",
    "\n",
    "\n",
    "\n",
    "**Deep Learning + 강화학습 ?**\n",
    "- $\\varepsilon$: Deep Neural Network \n",
    "- Value Function, policy: Boltzmann machines\n",
    "- divergence: gradient temporal-difference \n",
    "  (nonlinear function approximator, restricted variant) \n",
    "  \n",
    "\n",
    "**NFQ**: Neural fitted Q-Learning \n",
    "- RPROP 알고리즘 \n",
    "- batch update: 많은 computational cost가 발생합니다.   \n",
    "- autoencoder: 저차원으로 데이터로 시작하는 것이 raw visual input을 사용하는 것 보다 좋은 결과를 보입니다.   \n",
    "\n",
    "\n",
    ".   \n",
    "따라서...\n",
    "Atari2600에 다음과 같은 사항을 적용시켜 비교해 보게 됩니다.      \n",
    "- standard RL with linear function approximation & generic visual features \n",
    "- a large number of features -> low-dimentional space \n",
    "- HyperNEAT evolutionary architecture \n",
    "- trained repeatedly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experience replay\n",
    "* 위와 같이 세팅한 Deep RL에는 reward와 Q-value 값이 엄청나게 커질 수 있기 때문에 stable한 SGD 업데이트가 어려워진다는 단점이 있습니다. \n",
    "* 또한 on-policy sample을 통해 update하게 되면 sample에 대한 의존성이 커져서    \n",
    "  policy가 수렴하지 못하고 진동할 수 있습니다.\n",
    "* 이를 해결하기 위한 논문의 핵심 idea가 **experience replay** 입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"1140\" alt=\"rl2\" src=\"https://user-images.githubusercontent.com/43749571/77389611-fccddc00-6dd6-11ea-8be2-6d07f09b01e3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. each time step마다 agent의 experience를 저장합니다.    \n",
    "    - $e_t = (s_t, a_t, r_t, s_{t+1})$ (experience)\n",
    "    - $D = e_1, ... , e_N$ (튜플 형태로 마지막 N개 데이터 저장)\n",
    "    - **replay memory**\n",
    "2. sample을 업데이트 합니다. \n",
    "    - Q-learning updates, minibatch updates \n",
    "3. experience replay가 끝난 후, $\\varepsilon$-greedy policy를 사용해 action을 선택합니다. \n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "experience replay를 수행함으로써, \n",
    "- behavior distribution은 이전 state들의 평균을 기반으로 한 분포를 사용하게 됩니다. \n",
    "- 따라서 학습을 smoothing out 할 수 있으며, \n",
    "- 진동하거나 발산하는 가능성을 방지해 줍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning\n",
    "1. data efficiency:    \n",
    "모든 experience들이 weight update 되는 데에 계속 reuse 되기 때문에,    \n",
    "experience로 weight update를 한 번만 진행하는 것 보다 훨씬 효율적입니다. \n",
    "2. break correlation   \n",
    "데이터 특성 상 연속적인 sample끼리는 corr이 강합니다.   \n",
    "ramdom하게 sample을 뽑아 minibatch로 구성하기 때문에,   \n",
    "update들의 variance를 줄일 수 있습니다.   \n",
    "3. determine next samples   \n",
    "다음 training을 위한 data sample을 어느 정도 결정할 수 있습니다.   \n",
    "예를 들어 지금 왼쪽으로 가도록 action을 고른다면,    \n",
    "다음 sample들은 왼쪽에 있는 sample들이 주로 나올 것이라고 예측할 수 있습니다.   \n",
    "따라서 현재 action을 고려하여 효율적으로 뽑을 수 있습니다.  \n",
    "\n",
    "\n",
    "(주의점: off-policy로 학습해야 합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "- gray-scale & down-sampling (110* 84)\n",
    "- GPU 환경에 맞게 square로 crop 합니다 \n",
    "- last 4 frame을 stack으로 쌓아서, $\\phi$의 input data로 넣습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![r4](https://user-images.githubusercontent.com/43749571/77389624-035c5380-6dd7-11ea-9270-4cc89427b9e8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"745\" alt=\"rl3\" src=\"https://user-images.githubusercontent.com/43749571/77389620-00f9f980-6dd7-11ea-921c-68f4eed88306.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 모델은 $Q^*$를 단 한번의 forward pass만으로 구할 수 있다는 장점이 있습니다. \n",
    "- input: $\\phi(s_t)$\n",
    "- ouput: 가능한 모든 action에 대한 Q-value (4~18)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**experiment result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"1061\" alt=\"r5\" src=\"https://user-images.githubusercontent.com/43749571/77389626-03f4ea00-6dd7-11ea-964c-c81cf06ebfc9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward: (1,0,-1) 값으로 고정 \n",
    "- RMSProp algorithm with minibatches of size 32 \n",
    "- $\\varepsilon$-greedy policy: $\\varepsilon$ = 1 to 0.1 (~백만), $\\varepsilon$ = 0.1 (o.w)\n",
    "- a simple frame-skipping technique (agent가 on every kth frame에만 action을 선택) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논문에서 주목할만한 4가지 contribution     \n",
    "1. raw pixel을 받아와 directly input data로 다룬 것\n",
    "2. CNN을 function approximator로 이용한 것\n",
    "3. 하나의 agent가 여러 종류의 Atari game을 학습할 수 있는 능력을 갖춘 것\n",
    "4. Experience replay를 사용하여 data efficiency를 향상한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고자료\n",
    "1. 12기 유능한 멘토님들 (+ 기오님 강의자료 짱)\n",
    "2. http://sanghyukchun.github.io/90/\n",
    "3. https://hci-project.gitbook.io/reinforcement-learning-of-n-puzzle/n-puzzle-problem \n",
    "4. https://sumniya.tistory.com/18\n",
    "5. https://sumniya.tistory.com/15 \n",
    "6. https://brunch.co.kr/@chris-song/62 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
